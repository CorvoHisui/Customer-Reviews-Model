{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_kZxh_3wLIRI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpMCqRUPLIRN",
        "outputId": "e0a206ac-f147-4b54-db31-445e6eaceaae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     id             dateAdded           dateUpdated  \\\n",
            "0  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
            "1  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
            "2  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
            "3  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
            "4  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
            "\n",
            "                                                name       asins   brand  \\\n",
            "0  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
            "1  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
            "2  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
            "3  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
            "4  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
            "\n",
            "                                          categories primaryCategories  \\\n",
            "0  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
            "1  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
            "2  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
            "3  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
            "4  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
            "\n",
            "                                           imageURLs  \\\n",
            "0  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
            "1  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
            "2  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
            "3  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
            "4  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
            "\n",
            "                                                keys  ...  \\\n",
            "0  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
            "1  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
            "2  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
            "3  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
            "4  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
            "\n",
            "                                    reviews.dateSeen reviews.doRecommend  \\\n",
            "0  2018-05-27T00:00:00Z,2017-09-18T00:00:00Z,2017...               False   \n",
            "1  2018-05-27T00:00:00Z,2017-07-07T00:00:00Z,2017...                True   \n",
            "2                               2018-05-27T00:00:00Z                True   \n",
            "3                               2018-10-09T00:00:00Z                True   \n",
            "4                               2018-05-27T00:00:00Z                True   \n",
            "\n",
            "    reviews.id reviews.numHelpful reviews.rating  \\\n",
            "0          NaN                  0              3   \n",
            "1          NaN                  0              5   \n",
            "2          NaN                  0              4   \n",
            "3  177283626.0                  3              5   \n",
            "4          NaN                  0              5   \n",
            "\n",
            "                                  reviews.sourceURLs  \\\n",
            "0  http://reviews.bestbuy.com/3545/5442403/review...   \n",
            "1  http://reviews.bestbuy.com/3545/5442403/review...   \n",
            "2  https://reviews.bestbuy.com/3545/5442403/revie...   \n",
            "3  https://redsky.target.com/groot-domain-api/v1/...   \n",
            "4  https://reviews.bestbuy.com/3545/5442403/revie...   \n",
            "\n",
            "                                        reviews.text  \\\n",
            "0  I thought it would be as big as small paper bu...   \n",
            "1  This kindle is light and easy to use especiall...   \n",
            "2  Didnt know how much i'd use a kindle so went f...   \n",
            "3  I am 100 happy with my purchase. I caught it o...   \n",
            "4  Solid entry level Kindle. Great for kids. Gift...   \n",
            "\n",
            "                                  reviews.title  reviews.username  \\\n",
            "0                                     Too small            llyyue   \n",
            "1  Great light reader. Easy to use at the beach            Charmi   \n",
            "2                           Great for the price      johnnyjojojo   \n",
            "3                                   A Great Buy           Kdperry   \n",
            "4      Solid entry-level Kindle. Great for kids       Johnnyblack   \n",
            "\n",
            "                                          sourceURLs  \n",
            "0  https://www.newegg.com/Product/Product.aspx%25...  \n",
            "1  https://www.newegg.com/Product/Product.aspx%25...  \n",
            "2  https://www.newegg.com/Product/Product.aspx%25...  \n",
            "3  https://www.newegg.com/Product/Product.aspx%25...  \n",
            "4  https://www.newegg.com/Product/Product.aspx%25...  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 24 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   id                   5000 non-null   object \n",
            " 1   dateAdded            5000 non-null   object \n",
            " 2   dateUpdated          5000 non-null   object \n",
            " 3   name                 5000 non-null   object \n",
            " 4   asins                5000 non-null   object \n",
            " 5   brand                5000 non-null   object \n",
            " 6   categories           5000 non-null   object \n",
            " 7   primaryCategories    5000 non-null   object \n",
            " 8   imageURLs            5000 non-null   object \n",
            " 9   keys                 5000 non-null   object \n",
            " 10  manufacturer         5000 non-null   object \n",
            " 11  manufacturerNumber   5000 non-null   object \n",
            " 12  reviews.date         5000 non-null   object \n",
            " 13  reviews.dateAdded    1052 non-null   object \n",
            " 14  reviews.dateSeen     5000 non-null   object \n",
            " 15  reviews.doRecommend  5000 non-null   bool   \n",
            " 16  reviews.id           29 non-null     float64\n",
            " 17  reviews.numHelpful   5000 non-null   int64  \n",
            " 18  reviews.rating       5000 non-null   int64  \n",
            " 19  reviews.sourceURLs   5000 non-null   object \n",
            " 20  reviews.text         5000 non-null   object \n",
            " 21  reviews.title        4987 non-null   object \n",
            " 22  reviews.username     4999 non-null   object \n",
            " 23  sourceURLs           5000 non-null   object \n",
            "dtypes: bool(1), float64(1), int64(2), object(20)\n",
            "memory usage: 903.4+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = './Kaggle/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display first few records\n",
        "print(df.head())\n",
        "\n",
        "# Check the dataset info\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seOtz7NCLIRP",
        "outputId": "806c5f16-d049-4989-cba1-ccf95cdd3652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 24 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   id                   5000 non-null   object \n",
            " 1   dateAdded            5000 non-null   object \n",
            " 2   dateUpdated          5000 non-null   object \n",
            " 3   name                 5000 non-null   object \n",
            " 4   asins                5000 non-null   object \n",
            " 5   brand                5000 non-null   object \n",
            " 6   categories           5000 non-null   object \n",
            " 7   primaryCategories    5000 non-null   object \n",
            " 8   imageURLs            5000 non-null   object \n",
            " 9   keys                 5000 non-null   object \n",
            " 10  manufacturer         5000 non-null   object \n",
            " 11  manufacturerNumber   5000 non-null   object \n",
            " 12  reviews.date         5000 non-null   object \n",
            " 13  reviews.dateAdded    5000 non-null   object \n",
            " 14  reviews.dateSeen     5000 non-null   object \n",
            " 15  reviews.doRecommend  5000 non-null   bool   \n",
            " 16  reviews.id           29 non-null     float64\n",
            " 17  reviews.numHelpful   5000 non-null   int64  \n",
            " 18  reviews.rating       5000 non-null   int64  \n",
            " 19  reviews.sourceURLs   5000 non-null   object \n",
            " 20  reviews.text         5000 non-null   object \n",
            " 21  reviews.title        5000 non-null   object \n",
            " 22  reviews.username     4999 non-null   object \n",
            " 23  sourceURLs           5000 non-null   object \n",
            "dtypes: bool(1), float64(1), int64(2), object(20)\n",
            "memory usage: 903.4+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values by filling or dropping columns (optional)\n",
        "df.loc[:, 'reviews.title'] = df['reviews.title'].fillna('No title')  # Fill missing titles\n",
        "df.loc[:, 'reviews.dateAdded'] = df['reviews.dateAdded'].fillna('Unknown')  # Fill missing dates\n",
        "\n",
        "# Convert 'dateAdded' and 'dateUpdated' to datetime\n",
        "df.loc[:, 'dateAdded'] = pd.to_datetime(df['dateAdded'], errors='coerce')\n",
        "df.loc[:, 'dateUpdated'] = pd.to_datetime(df['dateUpdated'], errors='coerce')\n",
        "\n",
        "# Display cleaned data info\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 0\n",
            "Dataset shape after removing duplicates: (4385, 24)\n",
            "\n",
            "Checking duplicates in 'reviews.text' column\n",
            "Number of duplicate reviews: 615\n",
            "Dataset shape after removing duplicate reviews: (4385, 24)\n",
            "Deduplicated dataset saved to ./Kaggle/Datafiniti_Amazon_Consumer_Reviews_Deduplicated.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicate_count = df_cleaned.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "# Remove duplicate rows\n",
        "df_no_duplicates = df_cleaned.drop_duplicates()\n",
        "print(f\"Dataset shape after removing duplicates: {df_no_duplicates.shape}\")\n",
        "\n",
        "text_columns = [col for col in df.columns if 'review' in col.lower() or 'text' in col.lower()]\n",
        "if text_columns:\n",
        "    review_col = text_columns[8]\n",
        "    print(f\"\\nChecking duplicates in '{review_col}' column\")\n",
        "    duplicate_reviews = df.duplicated(subset=[review_col]).sum()\n",
        "    print(f\"Number of duplicate reviews: {duplicate_reviews}\")\n",
        "    \n",
        "    # Remove rows with duplicate reviews\n",
        "    df_unique_reviews = df.drop_duplicates(subset=[review_col])\n",
        "    print(f\"Dataset shape after removing duplicate reviews: {df_unique_reviews.shape}\")\n",
        "\n",
        "# Save the deduplicated dataset\n",
        "output_path = './Kaggle/Datafiniti_Amazon_Consumer_Reviews_Deduplicated.csv'\n",
        "df_unique_reviews.to_csv(output_path, index=False)\n",
        "print(f\"Deduplicated dataset saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_xPn4eQLIRR",
        "outputId": "35d11590-3ee2-4685-8459-1898e1cec54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.94      0.75      0.83        20\n",
            "     neutral       0.48      0.73      0.58        15\n",
            "    positive       0.83      0.68      0.75        22\n",
            "\n",
            "    accuracy                           0.72        57\n",
            "   macro avg       0.75      0.72      0.72        57\n",
            "weighted avg       0.78      0.72      0.73        57\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "df = df_unique_reviews\n",
        "\n",
        "# Map ratings to sentiment\n",
        "sentiment_mapping = {1: \"negative\", 2: \"negative\", 3: \"neutral\", 4: \"positive\", 5: \"positive\"}\n",
        "df[\"sentiment\"] = df[\"reviews.rating\"].map(sentiment_mapping)\n",
        "\n",
        "# Drop missing values\n",
        "df = df.dropna(subset=[\"reviews.text\", \"sentiment\"])\n",
        "\n",
        "# Handle class imbalance\n",
        "positive = df[df[\"sentiment\"] == \"positive\"]\n",
        "neutral = df[df[\"sentiment\"] == \"neutral\"]\n",
        "negative = df[df[\"sentiment\"] == \"negative\"]\n",
        "\n",
        "# Resample to balance classes\n",
        "min_samples = min(len(positive), len(neutral), len(negative))\n",
        "positive_downsampled = resample(positive, replace=False, n_samples=min_samples, random_state=42)\n",
        "neutral_downsampled = resample(neutral, replace=True, n_samples=min_samples, random_state=42)\n",
        "negative_upsampled = resample(negative, replace=True, n_samples=min_samples, random_state=42)\n",
        "\n",
        "# Combine balanced dataset\n",
        "balanced_df = pd.concat([positive_downsampled, neutral_downsampled, negative_upsampled])\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(balanced_df[\"reviews.text\"], balanced_df[\"sentiment\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nid41BYLIRU"
      },
      "source": [
        "# Data Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfWcnfDkLIRW",
        "outputId": "73e7676f-a7a4-4226-ec1c-226b7e814539"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "[WinError 2] The system cannot find the file specified\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"c:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
            "    cpu_info = subprocess.run(\n",
            "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
            "        capture_output=True,\n",
            "        text=True,\n",
            "    )\n",
            "  File \"C:\\Users\\rfria\\scoop\\apps\\python\\current\\Lib\\subprocess.py\", line 556, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\rfria\\scoop\\apps\\python\\current\\Lib\\subprocess.py\", line 1038, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        pass_fds, cwd, env,\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "                        gid, gids, uid, umask,\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        start_new_session, process_group)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\rfria\\scoop\\apps\\python\\current\\Lib\\subprocess.py\", line 1550, in _execute_child\n",
            "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
            "                             # no special security\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<4 lines>...\n",
            "                             cwd,\n",
            "                             ^^^^\n",
            "                             startupinfo)\n",
            "                             ^^^^^^^^^^^^\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Clustering Evaluation Metrics ---\n",
            "Silhouette Score: 0.0538\n",
            "Davies-Bouldin Index: 4.3277\n",
            "Calinski-Harabasz Index: 115.4831\n",
            "\n",
            "Cluster Size Distribution:\n",
            "Cluster 0: 882 items (20.11%)\n",
            "Cluster 1: 691 items (15.76%)\n",
            "Cluster 2: 976 items (22.26%)\n",
            "Cluster 3: 267 items (6.09%)\n",
            "Cluster 4: 979 items (22.33%)\n",
            "Cluster 5: 590 items (13.45%)\n",
            "\n",
            "Top Terms Per Cluster:\n",
            "Cluster 0: black, kindle, offers, special, tablet, includes, wifi, display, gb, ereader\n",
            "Cluster 1: kids, case, edition, kidproof, tablet, blue, gb, display, wifi, green\n",
            "Cluster 2: speaker, bluetooth, alexaenabled, echo, screen, amazon, tap, portable, love, great\n",
            "Cluster 3: ips, brand, new, gb, kindle, blue, tablet, amazon, display, wifi\n",
            "Cluster 4: hd, allnew, tablet, magenta, includes, special, offers, gb, display, wifi\n",
            "Cluster 5: plus, hub, builtin, silver, echo, amazon, alexa, love, great, lights\n",
            "\n",
            "Category Coherence:\n",
            "\n",
            "Category: Ebook Readers (Cluster 0)\n",
            "Number of items: 882\n",
            "Top terms: black, kindle, offers, special, tablet\n",
            "Sample product names: Fire Tablet, 7 Display, Wi-Fi, 16 GB - Includes Special Offers, Black, Amazon 9W PowerFast Official OEM USB Charger and Power Adapter for Fire Tablets and Kindle eReaders, Fire Tablet, 7 Display, Wi-Fi, 16 GB - Includes Special Offers, Black\n",
            "\n",
            "Category: Batteries (Cluster 1)\n",
            "Number of items: 691\n",
            "Top terms: kids, case, edition, kidproof, tablet\n",
            "Sample product names: Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16 GB, Green Kid-Proof Case, Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16 GB, Blue Kid-Proof Case, Fire Kids Edition Tablet, 7 Display, Wi-Fi, 16 GB, Blue Kid-Proof Case\n",
            "\n",
            "Category: Accessories (Cluster 2)\n",
            "Number of items: 976\n",
            "Top terms: speaker, bluetooth, alexaenabled, echo, screen\n",
            "Sample product names: Amazon Echo Show Alexa-enabled Bluetooth Speaker with 7\" Screen, Amazon Echo Show Alexa-enabled Bluetooth Speaker with 7\" Screen, Amazon Echo Show Alexa-enabled Bluetooth Speaker with 7\" Screen\n",
            "\n",
            "Category: Non-Electronics (Cluster 3)\n",
            "Number of items: 267\n",
            "Top terms: ips, brand, new, gb, kindle\n",
            "Sample product names: Brand New Amazon Kindle Fire 16gb 7\" Ips Display Tablet Wifi 16 Gb Blue, Brand New Amazon Kindle Fire 16gb 7\" Ips Display Tablet Wifi 16 Gb Blue, Brand New Amazon Kindle Fire 16gb 7\" Ips Display Tablet Wifi 16 Gb Blue\n",
            "\n",
            "Category: Cables & Adapters (Cluster 4)\n",
            "Number of items: 979\n",
            "Top terms: hd, allnew, tablet, magenta, includes\n",
            "Sample product names: All-New Fire HD 8 Tablet, 8\" HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta, Fire HD 10 Tablet, 10.1 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Silver Aluminum, All-New Fire HD 8 Tablet, 8\" HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta\n",
            "\n",
            "Category: Smart Home Devices (Cluster 5)\n",
            "Number of items: 590\n",
            "Top terms: plus, hub, builtin, silver, echo\n",
            "Sample product names: Amazon - Echo Plus w/ Built-In Hub - Silver, Amazon - Echo Plus w/ Built-In Hub - Silver, Amazon - Echo Plus w/ Built-In Hub - Silver\n",
            "\n",
            "Clustering complete. Results saved to ./Kaggle/clustered_reviews.csv\n",
            "                                                name       category\n",
            "0  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "1  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "2  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "3  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "4  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "5  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "6  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "7  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "8  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n",
            "9  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  Ebook Readers\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"./Kaggle/Datafiniti_Amazon_Consumer_Reviews_Deduplicated.csv\")\n",
        "df = df.dropna(subset=[\"name\", \"reviews.text\"])\n",
        "\n",
        "# Preprocess text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation/numbers\n",
        "    return text\n",
        "\n",
        "df[\"cleaned_text\"] = df[\"name\"] + \" \" + df[\"reviews.text\"]\n",
        "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(clean_text)\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=8000)\n",
        "X_tfidf = vectorizer.fit_transform(df[\"cleaned_text\"])\n",
        "\n",
        "# Apply K-Means clustering\n",
        "num_clusters = 6\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "df[\"cluster\"] = kmeans.fit_predict(X_tfidf)\n",
        "\n",
        "# Evaluate clustering quality\n",
        "print(\"\\n--- Clustering Evaluation Metrics ---\")\n",
        "\n",
        "# 1. Silhouette Score (higher is better, range: -1 to 1)\n",
        "silhouette = silhouette_score(X_tfidf, df[\"cluster\"])\n",
        "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
        "\n",
        "# 2. Davies-Bouldin Index (lower is better)\n",
        "davies_bouldin = davies_bouldin_score(X_tfidf.toarray(), df[\"cluster\"])\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
        "\n",
        "# 3. Calinski-Harabasz Index (higher is better)\n",
        "calinski_harabasz = calinski_harabasz_score(X_tfidf.toarray(), df[\"cluster\"])\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.4f}\")\n",
        "\n",
        "# 4. Cluster size distribution\n",
        "cluster_sizes = Counter(df[\"cluster\"])\n",
        "print(\"\\nCluster Size Distribution:\")\n",
        "for cluster, size in sorted(cluster_sizes.items()):\n",
        "    print(f\"Cluster {cluster}: {size} items ({size/len(df)*100:.2f}%)\")\n",
        "\n",
        "# 5. Find top terms per cluster\n",
        "def get_top_terms_per_cluster(vectorizer, kmeans, n_terms=10):\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    cluster_terms = {}\n",
        "    \n",
        "    for i in range(kmeans.n_clusters):\n",
        "        # Get indices of documents in this cluster\n",
        "        cluster_docs = df[df[\"cluster\"] == i].index\n",
        "        if len(cluster_docs) == 0:\n",
        "            continue\n",
        "            \n",
        "        cluster_tfidf = X_tfidf[cluster_docs]\n",
        "        cluster_vec = np.array(cluster_tfidf.sum(axis=0)).flatten()\n",
        "\n",
        "        top_indices = cluster_vec.argsort()[-n_terms:][::-1]\n",
        "\n",
        "        top_terms = [terms[idx] for idx in top_indices]\n",
        "        cluster_terms[i] = top_terms\n",
        "    \n",
        "    return cluster_terms\n",
        "\n",
        "top_terms = get_top_terms_per_cluster(vectorizer, kmeans)\n",
        "print(\"\\nTop Terms Per Cluster:\")\n",
        "for cluster, terms in top_terms.items():\n",
        "    print(f\"Cluster {cluster}: {', '.join(terms)}\")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df[\"cluster\"], cmap='viridis', alpha=0.5)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.title('PCA Visualization of Clusters')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.savefig('./Kaggle/cluster_visualization.png')\n",
        "plt.close()\n",
        "\n",
        "inertia_values = []\n",
        "silhouette_values = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans_test = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans_test.fit_predict(X_tfidf)\n",
        "    inertia_values.append(kmeans_test.inertia_)\n",
        "    silhouette_values.append(silhouette_score(X_tfidf, cluster_labels))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_range, inertia_values, 'bo-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_range, silhouette_values, 'ro-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Method')\n",
        "plt.tight_layout()\n",
        "plt.savefig('./Kaggle/optimal_clusters.png')\n",
        "plt.close()\n",
        "\n",
        "# Map clusters to categories manually\n",
        "cluster_map = {\n",
        "    0: \"Ebook Readers\",\n",
        "    1: \"Batteries\",\n",
        "    2: \"Accessories\",\n",
        "    3: \"Non-Electronics\",\n",
        "    4: \"Cables & Adapters\",\n",
        "    5: \"Smart Home Devices\"\n",
        "}\n",
        "df[\"category\"] = df[\"cluster\"].map(cluster_map)\n",
        "\n",
        "# 8. Evaluate category coherence\n",
        "print(\"\\nCategory Coherence:\")\n",
        "for cluster, category in cluster_map.items():\n",
        "    cluster_df = df[df[\"cluster\"] == cluster]\n",
        "    if len(cluster_df) > 0:\n",
        "        print(f\"\\nCategory: {category} (Cluster {cluster})\")\n",
        "        print(f\"Number of items: {len(cluster_df)}\")\n",
        "        print(f\"Top terms: {', '.join(top_terms[cluster][:5])}\")\n",
        "        print(f\"Sample product names: {', '.join(cluster_df['name'].sample(min(3, len(cluster_df))).tolist())}\")\n",
        "\n",
        "# Save and preview results\n",
        "df.to_csv(\"./Kaggle/clustered_reviews.csv\", index=False)\n",
        "print(\"\\nClustering complete. Results saved to ./Kaggle/clustered_reviews.csv\")\n",
        "print(df[[\"name\", \"category\"]].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWr5w7WwLIRX"
      },
      "source": [
        "# blog post like article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YteMrkJMi_Q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model for article generation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rfria\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Generating articles for 6 categories...\n",
            "Processing category: Ebook Readers\n",
            "  Generating article...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Article generated: 128 characters\n",
            "Processing category: Accessories\n",
            "  Generating article...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Generate article\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Generating article...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m article = \u001b[43mgenerate_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m articles.append({\n\u001b[32m    108\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m: category,\n\u001b[32m    109\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtop_products\u001b[39m\u001b[33m\"\u001b[39m: top_products,\n\u001b[32m    110\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mworst_product\u001b[39m\u001b[33m\"\u001b[39m: worst_product,\n\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m\"\u001b[39m: article\n\u001b[32m    112\u001b[39m })\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Article generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(article)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m characters\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mgenerate_article\u001b[39m\u001b[34m(prompt, max_length)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_article\u001b[39m(prompt, max_length=\u001b[32m300\u001b[39m):\n\u001b[32m     28\u001b[39m     inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, max_length=\u001b[32m1024\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add some creativity\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Avoid repetition\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\transformers\\generation\\utils.py:2345\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2339\u001b[39m         input_ids=input_ids,\n\u001b[32m   2340\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2341\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2342\u001b[39m         **model_kwargs,\n\u001b[32m   2343\u001b[39m     )\n\u001b[32m   2344\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2346\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2350\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2351\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2352\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2355\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2356\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2357\u001b[39m         batch_size=batch_size,\n\u001b[32m   2358\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2364\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2365\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\transformers\\generation\\utils.py:3866\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3861\u001b[39m \u001b[38;5;66;03m# g. Prepare remaining data for the next iteration, including computing the stopping condition for\u001b[39;00m\n\u001b[32m   3862\u001b[39m \u001b[38;5;66;03m# beam search as a whole (as opposed to individual beams, i.e. `stopping_criteria`)\u001b[39;00m\n\u001b[32m   3863\u001b[39m \n\u001b[32m   3864\u001b[39m \u001b[38;5;66;03m# pluck the cache from the beam indices that will be used in the next iteration\u001b[39;00m\n\u001b[32m   3865\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3866\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_temporary_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_key_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flatten_beam_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunning_beam_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3869\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3871\u001b[39m cur_len = cur_len + \u001b[32m1\u001b[39m\n\u001b[32m   3872\u001b[39m this_peer_finished = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._beam_search_has_unfinished_sequences(\n\u001b[32m   3873\u001b[39m     running_beam_scores,\n\u001b[32m   3874\u001b[39m     beam_scores,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3881\u001b[39m     length_penalty,\n\u001b[32m   3882\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\transformers\\generation\\utils.py:3405\u001b[39m, in \u001b[36mGenerationMixin._temporary_reorder_cache\u001b[39m\u001b[34m(self, past_key_values, beam_idx)\u001b[39m\n\u001b[32m   3402\u001b[39m     past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n\u001b[32m   3403\u001b[39m \u001b[38;5;66;03m# Standard code path: use the `Cache.reorder_cache`\u001b[39;00m\n\u001b[32m   3404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3405\u001b[39m     \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m past_key_values\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\transformers\\cache_utils.py:1554\u001b[39m, in \u001b[36mEncoderDecoderCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreorder_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, beam_idx: torch.LongTensor):\n\u001b[32m   1553\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1555\u001b[39m     \u001b[38;5;28mself\u001b[39m.cross_attention_cache.reorder_cache(beam_idx)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rfria\\Projects\\ironhacks\\customer-reviews\\.env\\Lib\\site-packages\\transformers\\cache_utils.py:84\u001b[39m, in \u001b[36mCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx] != []:\n\u001b[32m     83\u001b[39m     device = \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx].device\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx] != []:\n\u001b[32m     86\u001b[39m     device = \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx].device\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "\n",
        "df = pd.read_csv(\"/content/clustered_reviews.csv\")\n",
        "\n",
        "df = df.dropna(subset=[\"reviews.text\", \"reviews.rating\", \"name\", \"category\"])\n",
        "\n",
        "print(\"Loading model for article generation...\")\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model with memory optimizations\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# Function to generate article with memory management\n",
        "def generate_article(prompt, max_length=800):\n",
        "    # Clear CUDA cache if using GPU\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            temperature=0.7,\n",
        "            no_repeat_ngram_size=3,\n",
        "            do_sample=True,\n",
        "            length_penalty=1.0\n",
        "        )\n",
        "        \n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Clean up to free memory\n",
        "        del inputs, outputs\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"  CUDA out of memory, trying with smaller context...\")\n",
        "            inputs = tokenizer(prompt[:len(prompt)//2], return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "            outputs = model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_length=max_length//2,\n",
        "                num_beams=2,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "# Process categories in smaller batches to manage memory\n",
        "articles = []\n",
        "unique_categories = df[\"category\"].unique()\n",
        "print(f\"Found {len(unique_categories)} unique categories\")\n",
        "\n",
        "# Process categories in batches\n",
        "batch_size = 2\n",
        "for i in range(0, len(unique_categories), batch_size):\n",
        "    batch_categories = unique_categories[i:i+batch_size]\n",
        "    print(f\"Processing batch {i//batch_size + 1}/{(len(unique_categories) + batch_size - 1)//batch_size}\")\n",
        "    \n",
        "    for category in batch_categories:\n",
        "        print(f\"Processing category: {category}\")\n",
        "        cat_df = df[df[\"category\"] == category]\n",
        "        \n",
        "        # Skip if category has no products\n",
        "        if len(cat_df) == 0:\n",
        "            print(f\"  No products found in category {category}, skipping...\")\n",
        "            continue\n",
        "        \n",
        "        # Identify top 3 and worst product\n",
        "        product_ratings = cat_df.groupby(\"name\")[\"reviews.rating\"].mean().sort_values()\n",
        "        \n",
        "        # Skip if no products with ratings\n",
        "        if len(product_ratings) == 0:\n",
        "            print(f\"  No products with ratings in category {category}, skipping...\")\n",
        "            continue\n",
        "        \n",
        "        worst_product = product_ratings.index[0]\n",
        "        \n",
        "        # Get top products (up to 3)\n",
        "        if len(product_ratings) >= 3:\n",
        "            top_products = product_ratings.index[-3:].tolist()\n",
        "        else:\n",
        "            top_products = product_ratings.index.tolist()\n",
        "        \n",
        "        # Extract key positive points for top products (more memory efficient)\n",
        "        positive_points = {}\n",
        "        for product in top_products:\n",
        "            product_df = cat_df[cat_df[\"name\"] == product]\n",
        "            positive_reviews = product_df[product_df[\"reviews.rating\"] >= 4][\"reviews.text\"]\n",
        "            if len(positive_reviews) > 0:\n",
        "                # Take fewer samples to reduce memory usage\n",
        "                sample_size = min(3, len(positive_reviews))\n",
        "                positive_text = \" \".join(positive_reviews.sample(sample_size).tolist())\n",
        "                positive_points[product] = positive_text[:200]  # Shorter text\n",
        "        \n",
        "        # Extract key negative points for worst product\n",
        "        negative_points = \"\"\n",
        "        worst_product_df = cat_df[cat_df[\"name\"] == worst_product]\n",
        "        negative_reviews = worst_product_df[worst_product_df[\"reviews.rating\"] <= 2][\"reviews.text\"]\n",
        "        if len(negative_reviews) > 0:\n",
        "            sample_size = min(2, len(negative_reviews))\n",
        "            negative_points = \" \".join(negative_reviews.sample(sample_size).tolist())[:200]\n",
        "        \n",
        "        # Create prompt for article generation (simplified)\n",
        "        article_prompt = f\"\"\"\n",
        "        Write a detailed blog post about Amazon products in the category '{category}'.\n",
        "        \n",
        "        FORMAT:\n",
        "        - Title: [Catchy title]\n",
        "        - Introduction\n",
        "        - Best Products: {', '.join(top_products)}\n",
        "        - Worst Product: {worst_product}\n",
        "        - Conclusion\n",
        "        \n",
        "        Top product features: {str(positive_points)[:500]}\n",
        "        Issues with worst product: {negative_points}\n",
        "        \n",
        "        Make the article informative and helpful for consumers.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Generate article\n",
        "        print(f\"  Generating article...\")\n",
        "        article = generate_article(article_prompt, max_length=800)\n",
        "        \n",
        "        articles.append({\n",
        "            \"category\": category,\n",
        "            \"top_products\": top_products,\n",
        "            \"worst_product\": worst_product,\n",
        "            \"article\": article\n",
        "        })\n",
        "        print(f\"  Article generated: {len(article)} characters\")\n",
        "        \n",
        "        # Save progress after each category to avoid losing work\n",
        "        temp_df = pd.DataFrame(articles)\n",
        "        temp_df.to_csv(f\"/content/category_blog_articles_progress.csv\", index=False)\n",
        "        \n",
        "        # Force garbage collection\n",
        "        gc.collect()\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        # Add a small delay to let system recover\n",
        "        time.sleep(2)\n",
        "    \n",
        "    # Clear memory between batches\n",
        "    gc.collect()\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "articles_df = pd.DataFrame(articles)\n",
        "articles_df.to_csv(\"/content/category_blog_articles.csv\", index=False)\n",
        "\n",
        "print(\"\\nArticle generation complete!\")\n",
        "print(f\"Generated {len(articles)} articles\")\n",
        "print(\"Articles saved to /content/category_blog_articles.csv\")\n",
        "\n",
        "# Preview a sample article\n",
        "if len(articles) > 0:\n",
        "    sample_idx = 0\n",
        "    print(\"\\nSample Article Preview:\")\n",
        "    print(f\"Category: {articles[sample_idx]['category']}\")\n",
        "    print(f\"Top Products: {', '.join(articles[sample_idx]['top_products'])}\")\n",
        "    print(f\"Worst Product: {articles[sample_idx]['worst_product']}\")\n",
        "    print(\"\\nArticle:\")\n",
        "    print(articles[sample_idx]['article'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e560dda918040f7baa9a58d26334d41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eab5bc8f4094c83a685913d0b7efb1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97c7e6a70f16444e83e68feedff29e7c",
            "placeholder": "",
            "style": "IPY_MODEL_b1ac373626f64edaa356f5bafa9cd5da",
            "value": "Map:100%"
          }
        },
        "11a76697715449edbe0dd9f72590da58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2deb4db18f3845c4b56039244bc081a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5331512679a0449dbf6ef3aff8be5c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a38ca0be56ae4ecb86a87ecb948be3e9",
            "placeholder": "",
            "style": "IPY_MODEL_b873ec31b43d44b48b31fb70e5ecdba5",
            "value": "1000/1000[00:00&lt;00:00,2566.33examples/s]"
          }
        },
        "57d70af38127470cb4fd551f5e599ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d0cf43c93f4712b9d4e97d7b1cf06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f1ebe6eba7748c094ccf8f7a3f5a516",
              "IPY_MODEL_9fadc9c15c584b12a3241248e1354a17",
              "IPY_MODEL_5331512679a0449dbf6ef3aff8be5c37"
            ],
            "layout": "IPY_MODEL_0e560dda918040f7baa9a58d26334d41"
          }
        },
        "6f1ebe6eba7748c094ccf8f7a3f5a516": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9e79a1e761f4456b736088f9fff9e88",
            "placeholder": "",
            "style": "IPY_MODEL_8f312127dadb4fbd9b0f414975554ce1",
            "value": "Map:100%"
          }
        },
        "6f96c1856c5f481f962b91f2a61c0e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d3dd445f63425da3860d833d078bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78127709639a4a61bba13223a24d1db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0eab5bc8f4094c83a685913d0b7efb1d",
              "IPY_MODEL_fefe2a57d2144a0d850942a01a31d1da",
              "IPY_MODEL_985086daecc943cc886485072230ca5c"
            ],
            "layout": "IPY_MODEL_57d70af38127470cb4fd551f5e599ae8"
          }
        },
        "7f9ff8ae046246e3b59a2992fa3e3840": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f312127dadb4fbd9b0f414975554ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97c7e6a70f16444e83e68feedff29e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "985086daecc943cc886485072230ca5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72d3dd445f63425da3860d833d078bbb",
            "placeholder": "",
            "style": "IPY_MODEL_7f9ff8ae046246e3b59a2992fa3e3840",
            "value": "4000/4000[00:01&lt;00:00,2580.41examples/s]"
          }
        },
        "9fadc9c15c584b12a3241248e1354a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6bc6a4573a44b9fa62df8b2f9c71702",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11a76697715449edbe0dd9f72590da58",
            "value": 1000
          }
        },
        "a38ca0be56ae4ecb86a87ecb948be3e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ac373626f64edaa356f5bafa9cd5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b873ec31b43d44b48b31fb70e5ecdba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9e79a1e761f4456b736088f9fff9e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6bc6a4573a44b9fa62df8b2f9c71702": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fefe2a57d2144a0d850942a01a31d1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f96c1856c5f481f962b91f2a61c0e8e",
            "max": 4000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2deb4db18f3845c4b56039244bc081a2",
            "value": 4000
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
